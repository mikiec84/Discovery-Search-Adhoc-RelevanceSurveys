---
title: "Search Relevance Surveys"
subtitle: "Judging With Crowd-sourced Opinions and Machine Learning"
author:
  - "<a href='https://meta.wikimedia.org/wiki/User:EBernhardson_(WMF)'>Erik Bernhardson</a>"
  - "<a href='https://meta.wikimedia.org/wiki/User:TJones_(WMF)'>Trey Jones</a>"
  - "<a href='https://meta.wikimedia.org/wiki/User:MPopov_(WMF)'>Mikhail Popov</a>"
  - "<a href='https://meta.wikimedia.org/wiki/User:DTankersley_(WMF)'>Deb Tankersley</a>"
date: "`r format(Sys.Date(), '%d %B %Y')`"
abstract: >
  ...
output:
  html_document:
    # Table of Contents
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
    toc_depth: 3
    code_folding: hide
    # Figures
    fig_width: 12
    fig_height: 6
    # Theme
    theme: readable
    # Files
    self_contained: false
    keep_md: false
    # Extras
    mathjax: https://tools-static.wmflabs.org/cdnjs/ajax/libs/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML
    md_extensions: +raw_html +markdown_in_html_blocks +tex_math_dollars +fancy_lists +startnum +lists_without_preceding_blankline +table_captions
---
```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(knitr)
library(magrittr)
library(glue)
library(ggplot2)
if (is.null(opts_knit$get("rmarkdown.pandoc.to"))) {
  setwd("docs")
}
```
```{css, echo=FALSE}
@import url('https://fonts.googleapis.com/css?family=Source+Code+Pro|Source+Sans+Pro|Source+Serif+Pro');
body, p {
  font-family: 'Source Serif Pro', serif;
  font-size: 12pt;
}
pre, code {
  font-family: 'Source Code Pro', monospace;
}
table, tr, td, h1, h2, h3, h4, h5, h6 {
  font-family: 'Source Sans Pro', sans-serif;
}
.caption, caption {
  color: #2c3e50;
  font-size: 10pt;
  width: 90%;
  margin: 5px auto;
  text-align: left;
}
p.abstract {
  font-family: 'Source Sans Pro', sans-serif;
  font-weight: bold;
  font-size: 14pt !important;
}
.footnotes {
  margin-bottom: 80%;
}
```
```{js, echo=FALSE}
$( function() {
  /* Lets the user click on the images to view them in full resolution. */
  $( "img" ).wrap( function() {
    var link = $( '<a/>' );
    link.attr( 'href', $( this ).attr( 'src' ));
    link.attr( 'target', '_blank' );
    return link;
  } );
} );
$("p.abstract").text("Executive Summary");
```
<p style="text-align: center;"><a title="By Github project phacility/phabricator & w:de:User:Perhelion [Apache License 2.0 (http://www.apache.org/licenses/LICENSE-2.0)], via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File%3AFavicon-Phabricator-WM.png"><img width="16" alt="Favicon-Phabricator-WM" src="https://upload.wikimedia.org/wikipedia/commons/7/72/Favicon-Phabricator-WM.png"/></a> <a href="https://phabricator.wikimedia.org/T175048" title="T175048">Phabricator ticket</a> | <a title="By The Open Source Initiative [CC BY 2.5 (http://creativecommons.org/licenses/by/2.5)], via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File%3AOpen_Source_Initiative_keyhole.svg"><img width="16" alt="Open Source Initiative keyhole" src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/Open_Source_Initiative_keyhole.svg/16px-Open_Source_Initiative_keyhole.svg.png"/></a> <a href="https://github.com/wikimedia-research/Discovery-Search-Adhoc-RelevanceSurveys">Open source analysis</a> | <a title="Font Awesome by Dave Gandy - http://fortawesome.github.com/Font-Awesome [CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0)], via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File%3ADownload_font_awesome.svg"><img width="16" alt="Download font awesome" src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Download_font_awesome.svg/16px-Download_font_awesome.svg.png"/></a> <a href="https://github.com/wikimedia-research/Discovery-Search-Adhoc-RelevanceSurveys/blob/master/data">Open data</a></p>

## Introduction

### Discernatron

![Discernatron example](figures/example_discernatron.png)

[Discernatron](https://www.mediawiki.org/wiki/Discernatron) is a search relevance tool developed by the Discovery department. Its goal is to help improve search relevance - showing articles that are most relevant to search queries - with human assistance. We asked for people to use Discernatron to review search suggestions across multiple search tools. Respondents will be presented a set of search results from four tools - [CirrusSearch](https://www.mediawiki.org/wiki/CirrusSearch) ([Wiki search](https://www.mediawiki.org/wiki/Help:Searching)), Bing, Google, and DuckDuckGo. For each query users will be provided the set of titles found asked to rank each title from 1 to 4, or leave the title unranked.

| Labels | Label  | Score Range |
|:------:|:------:|:-----------:|
| k = 2  | bad    | [0, 1)      |
| k = 2  | good   | [1, 3]      |
| k = 3  | bad    | [0, 1)      |
| k = 3  | okay   | [1, 2)      |
| k = 3  | good   | [2, 3]      |
| k = 5  | worst  | 0           |
| k = 5  | worse  | (0, 1)      |
| k = 5  | okay   | [1, 2)      |
| k = 5  | better | [2, 3)      |
| k = 5  | best   | 3           |

Table: How Discernatron scores were discretized into relevance labels.

## Methods

The classifiers trained are:

- ([multinomial](https://en.wikipedia.org/wiki/Multinomial_logistic_regression)) [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), generically coded as "multinom" in the Results tables
- shallow [neural network](https://en.wikipedia.org/wiki/Artificial_neural_network) (with 1 hidden layer), coded as "nnet" in the Results tables
- [deep neural network](https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks) with (with up to 3 hidden layers), coded as "dnn" in the Results tables
- [na√Øve Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier), coded as "nb" in the Results tables
- [random forest](https://en.wikipedia.org/wiki/Random_forest), coded as "rf" in the Results tables
- [gradient-boosted trees](https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting) (via [XGBoost](https://github.com/dmlc/xgboost)), coded as "xgbTree" in the Results tables
- [C5.0](https://en.wikipedia.org/wiki/C4.5_algorithm) trees, coded as "C5.0" in the Results tables

In addition to the 7 classifiers listed above (which we will refer to as *base learners*), we also trained a super learner in a technique called *[stacking](https://en.wikipedia.org/wiki/Ensemble_learning#Stacking)*. The base learners are trained in the first stage and are then asked to make predictions. Those predictions are then used as features (one feature for each base learner) to train the super learner in the second stage. Specifically, we chose [Bayesian network](https://en.wikipedia.org/wiki/Bayesian_network) (via the [bnclassify](https://github.com/bmihaljevic/bnclassify) package) as the super learner, coded as "meta" in the Results tables.

We utilized the [caret](https://github.com/topepo/caret) package to perform hyperparameter tuning (via 5-fold cross-validation) and training (using 80% of the available data) of each base learner for a combination of each of the following:

- 4 questions we asked:
  - "If someone searched for '...', would they want to read this article?"
  - "If you searched for '...', would this article be a good result?"
  - "If you searched for '...', would this article be relevant?"
  - "Would you click on this page when searching for '...'?"
- 3 types of response: 2 vs 3 vs 5 labels
- 2 types of Discernatron scores that would be discretized into labels: *reliable* vs *unreliable*
  - reliability based on [Krippendorff's alpha](https://en.wikipedia.org/wiki/Krippendorff%27s_alpha) exceeding a threshold of 0.45
  - refer to [RelevanceScoring/Reliability.php](https://github.com/wikimedia/wikimedia-discovery-discernatron/blob/master/src/RelevanceScoring/Reliability.php) and [RelevanceScoring/KrippendorffAlpha.php](https://github.com/wikimedia/wikimedia-discovery-discernatron/blob/master/src/RelevanceScoring/KrippendorffAlpha.php) in Discernatron's source code for implementation details
- 14 different feature sets:
  - **survey-only**
    - score summarizing users' responses
    - proportion who answered unsure
    - engagement with the relevance survey
  - **survey & page info**
    - score, % unsure, engagement
    - page size label based on page byte length:
      - "tiny" (&le;1 kB)
      - "small" (1-10 kB)
      - "medium" (10-50 kB)
      - "large" (50-100 kB)
      - "huge" (&ge;100 kB)
    - indicator variables of whether the page is a:
      - Category page
      - Talk page
      - File page
      - list (e.g. "List of..."-type articles)
  - **survey & pageviews**
    - score, % unsure, engagement
    - median pageview (pv) traffic during September 2017, categorized as:
      - "no" (&le;1 pvs/day)
      - "low" (1-10 pvs/day)
      - "medium" (10-100 pvs/day)
      - "high" (100-1000 pvs/day)
      - "very high" (&ge;1000 pvs/day)
  - **survey, page info, and traffic**
    - score, % unsure, engagement
    - discrete page size and page type
  - **survey, page info, and traffic-by-weekday**
    - score, % unsure, engagement
    - discrete page size and page type
    - traffic on Monday-Sunday
  - **survey, page info, and traffic-by-platform**
    - score, % unsure, engagement
    - discrete page size and page type
    - traffic on desktop vs mobile web vs mobile app
  - **survey, page info, traffic-by-weekday, and traffic-by-platform**
    - score, % unsure, engagement
    - discrete page size and page type
    - traffic on Monday-Sunday
    - traffic on desktop vs mobile web vs mobile app
  - **survey, page info, and traffic-by-platform-and-weekday**
    - score, % unsure, engagement
    - discrete page size and page type
    - traffic on weekday from platform (7x3=21 combinations)
  - 6 configurations of **survey, page info, page size, and pageviews**
    - score, % unsure, engagement, page type
    - page size (in bytes) and pageviews (median/average in September 2017) using one of the following:
      - raw values
      - standardized (Z-score) raw values
      - normalized raw values
      - log<sub>10</sub>-transformed values
      - standardized (Z-score) log<sub>10</sub>-transformed values
      - normalized log<sub>10</sub>-transformed values

Standardization via means the predictor was centered around the mean and scaled by the standard deviation. Normalization was achieved via dividing by the maximum observed value and then subtracting 0.5 to center it around 0.

To correct for class imbalance, instances were [upsampled](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis).

## Results

```{r accuracy_data}
model_accuracy <- readr::read_tsv(file.path("..", "models", "model-accuracy.tsv"))
model_accuracy$features %<>% factor
model_accuracy$classifier %<>% factor
# model_accuracy$classifier %<>% factor(
#   c("meta", "multinom", "nb", "nnet", "dnn", "rf", "xgbTree", "C5.0"),
#   c("Meta (Bayesian Network)", "Logistic Regression", "Naive Bayes", "Shallow Neural Network", "Deep Neural Network", "Random Forest", "XGB trees", "C5.0 trees")
# )
model_accuracy$classes %<>% factor
```

### Marginal accuracies

```{r avg_question_accuracy, fig.width=12, fig.height=5}

model_accuracy %>%
  dplyr::filter(classes == 2) %>%
  dplyr::mutate(Reliability = dplyr::if_else(discernatron_reliable, "Reliable", "Unreliable")) %>%
  dplyr::group_by(Question = question, Reliability) %>%
  dplyr::summarize(
    median = median(accuracy),
    minimum = min(accuracy),
    maximum = max(accuracy)
  ) %>%
  dplyr::ungroup() %>%
  ggplot(aes(x = Question, color = Reliability)) +
  geom_linerange(
    aes(ymin = minimum, ymax = maximum),
    position = position_dodge(width = 0.6)
  ) +
  geom_label(
    aes(y = median, label = sprintf("%.1f%%", 100 * median)),
    position = position_dodge(width = 0.6)
  ) +
  scale_y_continuous(
    limits = c(0, 1), breaks = seq(0, 1, 0.1),
    labels = scales::percent_format()
  ) +
  scale_color_brewer(palette = "Set1") +
  coord_flip() +
  labs(
    x = NULL, y = "Accuracy", color = "Discernatron score reliability",
    title = "Performance by question and Discernatron score reliability",
    subtitle = paste(
      "Binary classification accuracy averaged across",
      prod(dim(table(model_accuracy$features, model_accuracy$classifier))),
      "feature set-classifier combinations"
    )
  ) +
  wmf::theme_min(14, "Source Sans Pro")
```

```{r avg_feature_accuracy, fig.width=12, fig.height=7}
model_accuracy %>%
  dplyr::filter(classes == 2) %>%
  dplyr::mutate(Reliability = dplyr::if_else(discernatron_reliable, "Reliable", "Unreliable")) %>%
  dplyr::group_by(Features = features, Reliability) %>%
  dplyr::summarize(
    median = median(accuracy),
    minimum = min(accuracy),
    maximum = max(accuracy)
  ) %>%
  dplyr::ungroup() %>%
  ggplot(aes(x = Features, color = Reliability)) +
  geom_linerange(
    aes(ymin = minimum, ymax = maximum),
    position = position_dodge(width = 0.8)
  ) +
  geom_label(
    aes(y = median, label = sprintf("%.1f%%", 100 * median)),
    position = position_dodge(width = 0.8),
    size = 4, label.padding = unit(0.15, "lines")
  ) +
  scale_y_continuous(
    limits = c(0, 1), breaks = seq(0, 1, 0.1),
    labels = scales::percent_format()
  ) +
  scale_color_brewer(palette = "Set1") +
  coord_flip() +
  labs(
    x = NULL, y = "Accuracy", color = "Discernatron score reliability",
    title = "Performance by feature set and Discernatron score reliability",
    subtitle = paste(
      "Binary classification accuracy averaged across",
      prod(dim(table(model_accuracy$question, model_accuracy$classifier))),
      "question-classifier combinations"
    )
  ) +
  wmf::theme_min(14, "Source Sans Pro")
```

```{r avg_stage_accuracy, results='asis'}
model_accuracy %>%
  dplyr::filter(classes == 2) %>%
  dplyr::mutate(
    Reliability = dplyr::if_else(discernatron_reliable, "Reliable Discernatron score", "Unreliable Discernatron score"),
    Classifier = factor(
      classifier,
      levels = c("multinom", "nnet", "dnn", "nb", "rf", "xgbTree", "C5.0", "meta"),
      labels = c("Logistic Regression (multinom)", "Shallow Neural Network (nnet)", "Deep Neural Network (dnn)", "Naive Bayes (nb)", "Random Forest (rf)", "Gradient-boosted Trees (xgbTree)", "C5.0 trees", "Bayesian Network super learner (meta)")
    )
  ) %>%
  dplyr::group_by(Classifier, Reliability) %>%
  dplyr::summarize(avg = sprintf("%.1f%%", mean(100 * accuracy))) %>%
  dplyr::ungroup() %>%
  tidyr::spread(Reliability, avg) %>%
  knitr::kable(format = "markdown")
```

Table: Binary classification performance by classifier and Discernatron score reliability, with accuracy averaged across `r prod(dim(table(model_accuracy$question, model_accuracy$features)))` question-feature set combinations.

### Accuracies by question

#### Question 1

##### Reliable Scores

```{r}
DT::datatable(
  dplyr::select(dplyr::filter(
    model_accuracy,
    question == "If someone searched for '...', would they want to read this article?",
    discernatron_reliable
  ), -c(question, discernatron_reliable)),
  filter = "top",
  extensions = "Buttons",
  options = list(
    pageLength = 10, autoWidth = TRUE, language = list(search = "Filter:"),
    order = list(list(4, "desc")), dom = "Bfrtip", buttons = c("copy", "csv")
  ),
  caption = "Accuracy of models trained on responses to the question \"If someone searched for '...', would they want to read this article?\" and relevance labels based on reliable Discernatron scores."
) %>% DT::formatPercentage("accuracy", 3)
```

##### Unreliable Scores

```{r}
DT::datatable(
  dplyr::select(dplyr::filter(
    model_accuracy,
    question == "If someone searched for '...', would they want to read this article?",
    !discernatron_reliable
  ), -c(question, discernatron_reliable)),
  filter = "top",
  extensions = "Buttons",
  options = list(
    pageLength = 10, autoWidth = TRUE, language = list(search = "Filter:"),
    order = list(list(4, "desc")), dom = "Bfrtip", buttons = c("copy", "csv")
  ),
  caption = "Accuracy of models trained on responses to the question \"If someone searched for '...', would they want to read this article?\" and relevance labels based on unreliable Discernatron scores."
) %>% DT::formatPercentage("accuracy", 3)
```

#### Question 2

##### Reliable Scores

```{r}
DT::datatable(
  dplyr::select(dplyr::filter(
    model_accuracy,
    question == "If you searched for '...', would this article be a good result?",
    discernatron_reliable
  ), -c(question, discernatron_reliable)),
  filter = "top",
  extensions = "Buttons",
  options = list(
    pageLength = 10, autoWidth = TRUE, language = list(search = "Filter:"),
    order = list(list(4, "desc")), dom = "Bfrtip", buttons = c("copy", "csv")
  ),
  caption = "Accuracy of models trained on responses to the question \"If you searched for '...', would this article be a good result?\" and relevance labels based on reliable Discernatron scores."
) %>% DT::formatPercentage("accuracy", 3)
```

##### Unreliable Scores

```{r}
DT::datatable(
  dplyr::select(dplyr::filter(
    model_accuracy,
    question == "If you searched for '...', would this article be a good result?",
    !discernatron_reliable
  ), -c(question, discernatron_reliable)),
  filter = "top",
  extensions = "Buttons",
  options = list(
    pageLength = 10, autoWidth = TRUE, language = list(search = "Filter:"),
    order = list(list(4, "desc")), dom = "Bfrtip", buttons = c("copy", "csv")
  ),
  caption = "Accuracy of models trained on responses to the question \"If you searched for '...', would this article be a good result?\" and relevance labels based on unreliable Discernatron scores."
) %>% DT::formatPercentage("accuracy", 3)
```

#### Question 3

##### Reliable Scores

```{r}
DT::datatable(
  dplyr::select(dplyr::filter(
    model_accuracy,
    question == "If you searched for '...', would this article be relevant?",
    discernatron_reliable
  ), -c(question, discernatron_reliable)),
  filter = "top",
  extensions = "Buttons",
  options = list(
    pageLength = 10, autoWidth = TRUE, language = list(search = "Filter:"),
    order = list(list(4, "desc")), dom = "Bfrtip", buttons = c("copy", "csv")
  ),
  caption = "Accuracy of models trained on responses to the question \"If you searched for '...', would this article be relevant?\" and relevance labels based on reliable Discernatron scores."
) %>% DT::formatPercentage("accuracy", 3)
```

##### Unreliable Scores

```{r}
DT::datatable(
  dplyr::select(dplyr::filter(
    model_accuracy,
    question == "If you searched for '...', would this article be relevant?",
    !discernatron_reliable
  ), -c(question, discernatron_reliable)),
  filter = "top",
  extensions = "Buttons",
  options = list(
    pageLength = 10, autoWidth = TRUE, language = list(search = "Filter:"),
    order = list(list(4, "desc")), dom = "Bfrtip", buttons = c("copy", "csv")
  ),
  caption = "Accuracy of models trained on responses to the question \"If you searched for '...', would this article be relevant?\" and relevance labels based on unreliable Discernatron scores."
) %>% DT::formatPercentage("accuracy", 3)
```

#### Question 4

##### Reliable Scores

```{r}
DT::datatable(
  dplyr::select(dplyr::filter(
    model_accuracy,
    question == "Would you click on this page when searching for '...'?",
    discernatron_reliable
  ), -c(question, discernatron_reliable)),
  filter = "top",
  extensions = "Buttons",
  options = list(
    pageLength = 10, autoWidth = TRUE, language = list(search = "Filter:"),
    order = list(list(4, "desc")), dom = "Bfrtip", buttons = c("copy", "csv")
  ),
  caption = "Accuracy of models trained on responses to the question \"Would you click on this page when searching for '...'?\" and relevance labels based on reliable Discernatron scores."
) %>% DT::formatPercentage("accuracy", 3)
```

##### Unreliable Scores

```{r}
DT::datatable(
  dplyr::select(dplyr::filter(
    model_accuracy,
    question == "Would you click on this page when searching for '...'?",
    !discernatron_reliable
  ), -c(question, discernatron_reliable)),
  filter = "top",
  extensions = "Buttons",
  options = list(
    pageLength = 10, autoWidth = TRUE, language = list(search = "Filter:"),
    order = list(list(4, "desc")), dom = "Bfrtip", buttons = c("copy", "csv")
  ),
  caption = "Accuracy of models trained on responses to the question \"Would you click on this page when searching for '...'?\" and relevance labels based on unreliable Discernatron scores."
) %>% DT::formatPercentage("accuracy", 3)
```

## Discussion

## References
